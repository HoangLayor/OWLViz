<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>OWLViz: An Open-World Benchmark for VQA</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
</head>
<body class="bg-white text-gray-800" style="font-family: 'Crimson Text', 'Times New Roman', Times, serif;">
  <div class="max-w-4xl mx-auto px-4 py-10">
    <div class="text-center">
      <h1 class="text-4xl font-bold mb-2">OWLViz: An Open-World Benchmark for Visual Question Answering</h1>
      <p class="text-lg">
        <a href="https://openreview.net/profile?id=~Thuy_Nguyen3" target="_blank" class="author-link">Thuy Nguyen</a><sup>1</sup>, 
        <a href="https://openreview.net/profile?id=~Dang_Nguyen3" target="_blank" class="author-link">Dang Nguyen</a><sup>2</sup>, 
        <a href="https://openreview.net/profile?id=~Nguyen_Khac_Gia_Hoang1" target="_blank" class="author-link">Hoang Nguyen</a><sup>3</sup>, 
        <a href="https://openreview.net/profile?id=~Thuan_Duc_Luong1" target="_blank" class="author-link">Thuan Luong</a><sup>3</sup>, 
        <br>
        <a href="https://openreview.net/profile?id=~Franck_Dernoncourt1" target="_blank" class="author-link">Franck Dernoncourt</a><sup>4</sup>, 
        <a href="https://openreview.net/profile?id=~Long_Hoang_Dang1" target="_blank" class="author-link">Long Hoang Dang</a><sup>3</sup>, 
        <a href="https://openreview.net/profile?id=~Viet_Dac_Lai1" target="_blank" class="author-link">Viet Dac Lai</a><sup>4</sup>
      </p>
      <p class="text-lg text-gray-600 mt-1">
        <sup>1</sup>Reasoning Foundation &emsp;
        <sup>2</sup>University of Maryland &emsp;
        <br>
        <sup>3</sup>Posts and Telecommunications Institute of Technology &emsp;
        <sup>4</sup>Adobe Research
      </p>
      <div class="flex justify-center space-x-3 my-4">
        <a href="https://arxiv.org/abs/2503.07631" target="_blank">
          <img alt="arXiv" src="https://img.shields.io/badge/arXiv-OWLViz-red?logo=arxiv" height="30">
        </a>
        <a href="https://github.com/dangne/viz_inspector" target="_blank">
          <img alt="GitHub repo" src="https://img.shields.io/badge/GitHub-Repository-blue?logo=github" height="30">
        </a>
        <a href="https://hoanglayor.github.io/OWLViz/" target="_blank">
          <img alt="HF Dataset" src="https://img.shields.io/badge/%F0%9F%93%9A%20Dataset-OWLViz-ff69b4" height="30">
        </a>
      </div>
    </div>
    
    <!-- Abstract Section -->
    <section class="mt-12 mb-16">
      <!-- <img src="docs/resources/teaser.png" class="my-6" alt="Teaser Image"> -->
      <div class="mx-8 md:mx-16">
        <p class="text-center text-lg leading-relaxed">
          OWLViz is a challenging open-world benchmark designed to evaluate Vision-Language Models and Agents in Visual Question Answering tasks that require multi-step reasoning, tool usage, and external knowledge retrieval. Unlike traditional VQA datasets, OWLViz questions are short, clear, and demand complex reasoning over degraded images and external information sources.
        </p>
      </div>
    </section>

    <!-- <section class="mt-16">
      <h2 class="heading">Introduction</h2>
      <div class="underline-gradient"></div>
      <p class="text-justify">
        This repository presents <strong>OWLViz: An Open-World Benchmark for Visual Question Answering</strong>. OWLViz is a <strong>challenging benchmark dataset</strong> designed to evaluate visual question answering capabilities in open-world scenarios. It requires the integration of multiple capabilities, including common-sense knowledge, visual understanding, web exploration, and specialized tool usage to answer short queries.
      </p>
    </section> -->

    <section class="mt-16">
      <h2 class="heading">OWLViz Dataset</h2>
      <div class="underline-gradient"></div>
      
      <!-- Dataset Size -->
      <div class="mb-8">
        <h3 class="text-xl font-semibold mb-3 text-gray-800">Dataset Size</h3>
        <p class="text-justify">OWLViz comprises <strong>248 carefully annotated questions and answers</strong>, designed to comprehensively evaluate multi-modal reasoning capabilities.</p>
      </div>

      <!-- Question Design -->
      <div class="mb-8">
        <h3 class="text-xl font-semibold mb-3 text-gray-800">Question Design</h3>
        <p class="text-justify mb-4">Each question is associated with one or more of the following skill categories:</p>
        <div class="grid md:grid-cols-3 gap-4 mb-4">
          <div class="bg-gray-50 p-4 rounded border">
            <h4 class="font-semibold text-gray-800 mb-2">Visual Skills</h4>
            <p class="text-sm">Recognition, segmentation, attribute identification, spatial relations</p>
          </div>
          <div class="bg-gray-50 p-4 rounded border">
            <h4 class="font-semibold text-gray-800 mb-2">Reasoning Skills</h4>
            <p class="text-sm">Measurement, arithmetic, logic, counting, comparison</p>
          </div>
          <div class="bg-gray-50 p-4 rounded border">
            <h4 class="font-semibold text-gray-800 mb-2">Tool Use Skills</h4>
            <p class="text-sm">API calls, OCR, GUI interaction, search</p>
          </div>
        </div>
        <p class="text-justify text-gray-600">Answers are provided in standardized formats like yes/no, multiple choice, and short text.</p>
      </div>

      <!-- Image Sources -->
      <div class="mb-8">
        <h3 class="text-xl font-semibold mb-3 text-gray-800">Image Sources</h3>
        <p class="text-justify">Images were gathered from publicly available websites and selected for their visual difficulty. They simulate realistic challenges such as <strong>low brightness, blur, and low contrast</strong>.</p>
      </div>

      <!-- Annotation Process -->
      <div class="mb-8">
        <h3 class="text-xl font-semibold mb-3 text-gray-800">Annotation Process</h3>
        <p class="text-justify">The dataset was designed and annotated by the authors through a <strong>three-phase process</strong> to ensure quality, solvability, and objectivity. This rigorous process ensures that only independently answerable questions, clearly grounded in the visual content, are included.</p>
      </div>

      <!-- Difficulty Levels -->
      <div class="mb-8">
        <h3 class="text-xl font-semibold mb-3 text-gray-800">Difficulty Levels</h3>
        <p class="text-justify mb-4">Questions are categorized into three increasing levels of difficulty based on the number of unique skills required:</p>
        <div class="text-lg space-y-4">
          <div class="border-l-4 border-green-500 pl-4 py-2">
            <h4 class="font-semibold text-green-700 mb-1">Level 1</h4>
            <p class="text-sm">Typically involves no more than 2 unique skills and at most 1 external tool.</p>
          </div>
          <div class="border-l-4 border-yellow-500 pl-4 py-2">
            <h4 class="font-semibold text-yellow-700 mb-1">Level 2</h4>
            <p class="text-sm">Generally between 3 and 5 unique skills, often including a combination of two tools.</p>
          </div>
          <div class="border-l-4 border-red-500 pl-4 py-2">
            <h4 class="font-semibold text-red-700 mb-1">Level 3</h4>
            <p class="text-sm">Designed for an ideal general-purpose assistant, these questions may require arbitrarily long action sequences, unrestricted tool use and general internet access.</p>
          </div>
        </div>
      </div>
      <div style="display: flex; justify-content: space-between; gap: 16px; margin-top: 20px;">
      
      <!-- Example 1 -->
      <div style="flex: 1;">
        <div style="text-align: center; margin-bottom: 12px;">
          <img src="assets/images/example1_lowlight_level1.jpg" alt="Example 1" style="height: 180px; object-fit: cover; display: block; margin: 0 auto;">
        </div>
        <p style="text-align: left; font-size: 14px;">(a) <strong>Question:</strong> <em>"How many people are visible on the left side of the white line that cuts across the photo? Provide a numeric answer."</em>
          <strong>Answer:</strong> 2<br>
          <strong>Skills:</strong> using external API, human recognition
          <strong>Difficulty level:</strong> 1
        </p>
      </div>

      <!-- Example 2 -->
      <div style="flex: 1;">
        <div style="text-align: center; margin-bottom: 12px;">
          <img src="assets/images/example2_umbrella_level2.jpg" alt="Example 2" style="height: 180px; object-fit: cover; display: block; margin: 0 auto;">
        </div>
        <p style="text-align: left; font-size: 14px;">(b) <strong>Question:</strong> <em>"How many umbrellas have 3 or more colors? Provide a numeric answer."</em>
          <strong>Answer:</strong> 2<br>
          <strong>Skills:</strong> object recognition, attribute identification, counting, object detection
          <strong>Difficulty level:</strong> 2
        </p>
      </div>

      <!-- Example 3 -->
      <div style="flex: 1;">
        <div style="text-align: center; margin-bottom: 12px;">
          <img src="assets/images/example3_roadname_level3.png" alt="Example 3" style="height: 180px; object-fit: cover; display: block; margin: 0 auto;">
        </div>
        <p style="text-align: left; font-size: 14px;">(c) <strong>Question:</strong> <em>"This is in Fairfax, Virginia. What is the name of the road shown in the photo?"</em>
          <strong>Answer:</strong> Shadowridge Dr; Shadowridge drive; Shadowridge<br>
          <strong>Skills:</strong> OCR, knowledge search, knowledge retrieval, GUI, comparison, spatial relationships
          <strong>Difficulty level:</strong> 3
        </p>
      </div>
    </div>

    <!-- Caption -->
    <p style="text-align: left; margin-top: 16px; font-size: 14px; color: #6b7280;">
      <strong>Figure: Examples of the three core challenges in our OWLViz dataset</strong>.
      (a) Challenging visual conditions requiring image enhancement or specialized recognition tools to count people on a white line in a low-contrast night scene.
      (b) Complex reasoning tasks demanding object detection, attribute identification, and precise counting of multi-colored umbrellas in a dynamic street scene.
      (c) Knowledge-intensive queries requiring internet exploration and external data retrieval to identify specific locations based on minimal visual cues.
    </p>

    </section>

    <section class="mt-16">
      <h2 class="heading">Challenges and Significance</h2>
      <div class="underline-gradient"></div>
      
      <!-- Introduction with visual separator -->
      <div class="text-center mb-12">
        <p class="text-xl italic text-gray-600 max-w-3xl mx-auto">
          OWLViz pushes the boundaries of multi-modal AI by testing three fundamental capabilities that current systems struggle with:
        </p>
        <div class="w-24 h-0.5 bg-gray-300 mx-auto mt-4"></div>
      </div>

      <!-- Three-column challenge layout -->
      <div class="grid md:grid-cols-3 gap-8 mb-12">
        <!-- Challenge 1: Visual -->
        <div class="text-center">
          <div class="mx-auto w-16 h-16 bg-gradient-to-br from-gray-100 to-gray-200 rounded-full flex items-center justify-center mb-4">
            <svg class="w-8 h-8 text-gray-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"></path>
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z"></path>
            </svg>
          </div>
          <h3 class="text-xl font-semibold mb-3">Visual Degradation</h3>
          <p class="text-base text-gray-600 mb-4">Processing low-quality, blurred, or poorly lit images that mirror real-world conditions</p>
          <div class="bg-gray-50 p-3 rounded text-sm italic border-l-2 border-gray-300">
            "Count people in a night scene with poor visibility"
          </div>
        </div>

        <!-- Challenge 2: Reasoning -->
        <div class="text-center">
          <div class="mx-auto w-16 h-16 bg-gradient-to-br from-gray-100 to-gray-200 rounded-full flex items-center justify-center mb-4">
            <svg class="w-8 h-8 text-gray-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.663 17h4.673M12 3v1m6.364 1.636l-.707.707M21 12h-1M4 12H3m3.343-5.657l-.707-.707m2.828 9.9a5 5 0 117.072 0l-.548.547A3.374 3.374 0 0014 18.469V19a2 2 0 11-4 0v-.531c0-.895-.356-1.754-.988-2.386l-.548-.547z"></path>
            </svg>
          </div>
          <h3 class="text-xl font-semibold mb-3">Complex Reasoning</h3>
          <p class="text-base text-gray-600 mb-4">Multi-step cognitive processes involving counting, measurement, and logical deduction</p>
          <div class="bg-gray-50 p-3 rounded text-sm italic border-l-2 border-gray-300">
            "Identify and count multi-colored umbrellas by attributes"
          </div>
        </div>

        <!-- Challenge 3: External Knowledge -->
        <div class="text-center">
          <div class="mx-auto w-16 h-16 bg-gradient-to-br from-gray-100 to-gray-200 rounded-full flex items-center justify-center mb-4">
            <svg class="w-8 h-8 text-gray-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 12a9 9 0 01-9 9m9-9a9 9 0 00-9-9m9 9H3m9 9v-9m0-9v9m0 9c-5 0-9-4-9-9s4-9 9-9"></path>
            </svg>
          </div>
          <h3 class="text-xl font-semibold mb-3">Web Exploration</h3>
          <p class="text-base text-gray-600 mb-4">Internet search and external data retrieval based on minimal visual cues</p>
          <div class="bg-gray-50 p-3 rounded text-sm italic border-l-2 border-gray-300">
            "Find road name in Fairfax using OCR and web search"
          </div>
        </div>
      </div>

      <!-- Connecting diagram -->
      <div class="text-center mb-8">
        <div class="inline-flex items-center space-x-2 text-base text-gray-500">
          <span>Visual Processing</span>
          <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"></path>
          </svg>
          <span>Reasoning</span>
          <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"></path>
          </svg>
          <span>Knowledge Integration</span>
        </div>
      </div>
    </section>

    <section class="mt-20">
      <h2 class="heading text-4xl">Experiments and Results</h2>
      <div class="underline-gradient"></div>
      
      <!-- Introduction -->
      <div class="text-center mb-16">
        <p class="text-2xl italic text-gray-700 max-w-4xl mx-auto leading-relaxed">
          Three methodological approaches were systematically evaluated: Vanilla VLMs, Tool-Calling Agents, and GUI Agents
        </p>
        <div class="w-32 h-0.5 bg-gray-400 mx-auto mt-6"></div>
      </div>

      <!-- Human Baseline -->
      <div class="bg-gradient-to-r from-blue-50 to-blue-100 rounded-xl p-8 mb-12 text-center shadow-sm">
        <div class="flex items-center justify-center mb-6">
          <div class="w-16 h-16 bg-blue-500 rounded-full flex items-center justify-center mr-6">
            <svg class="w-8 h-8 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 4.354a4 4 0 110 5.292M15 21H3v-1a6 6 0 0112 0v1zm0 0h6v-1a6 6 0 00-9-5.197m13.5-9a2.25 2.25 0 11-4.5 0 2.25 2.25 0 014.5 0z"></path>
            </svg>
          </div>
          <div class="text-left">
            <h3 class="text-2xl font-semibold text-blue-800 mb-2">Human Baseline</h3>
            <p class="text-3xl font-bold text-blue-900">69.2% Accuracy</p>
          </div>
        </div>
        <p class="text-lg text-blue-700 max-w-2xl mx-auto">Establishing the upper bound for model performance on these intuitive visual reasoning tasks</p>
      </div>

      <!-- Model Performance Grid -->
      <div class="grid md:grid-cols-3 gap-8 mb-12">
        <!-- Vanilla VLMs -->
        <div class="border border-gray-200 rounded-xl p-8 bg-white shadow-sm hover:shadow-md transition-shadow">
          <div class="text-center mb-6">
            <div class="w-16 h-16 bg-red-100 rounded-full flex items-center justify-center mx-auto mb-4">
              <svg class="w-8 h-8 text-red-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"></path>
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z"></path>
              </svg>
            </div>
            <h3 class="text-xl font-semibold text-gray-800 mb-2">Vanilla VLMs</h3>
          </div>
          <div class="space-y-4">
            <div class="bg-red-50 p-4 rounded-lg border-l-4 border-red-400">
              <p class="text-base font-semibold text-red-800 mb-1">Best: Gemini</p>
              <p class="text-2xl font-bold text-red-900">27.09% LM</p>
            </div>
            <p class="text-base text-gray-600 leading-relaxed">Struggled with multi-step reasoning and tool use. Most models scored below 20% EM and 30% LM.</p>
          </div>
        </div>

        <!-- Tool-Calling Agents -->
        <div class="border border-gray-200 rounded-xl p-8 bg-white shadow-sm hover:shadow-md transition-shadow">
          <div class="text-center mb-6">
            <div class="w-16 h-16 bg-yellow-100 rounded-full flex items-center justify-center mx-auto mb-4">
              <svg class="w-8 h-8 text-yellow-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M11 4a2 2 0 114 0v1a1 1 0 001 1h3a1 1 0 011 1v3a1 1 0 01-1 1h-1a2 2 0 100 4h1a1 1 0 011 1v3a1 1 0 01-1 1h-3a1 1 0 01-1-1v-1a2 2 0 10-4 0v1a1 1 0 01-1 1H7a1 1 0 01-1-1v-3a1 1 0 00-1-1H4a1 1 0 01-1-1V9a1 1 0 011-1h1a2 2 0 100-4H4a1 1 0 01-1-1V4a1 1 0 011-1h3a1 1 0 001-1z"></path>
              </svg>
            </div>
            <h3 class="text-xl font-semibold text-gray-800 mb-2">Tool-Calling Agents</h3>
          </div>
          <div class="space-y-4">
            <div class="bg-yellow-50 p-3 rounded-lg text-sm">
              <span class="font-semibold">HF Agent:</span> 18.32% EM
            </div>
            <div class="bg-yellow-50 p-3 rounded-lg text-sm">
              <span class="font-semibold">DynaSaur:</span> 16.23% EM, 26.67% LM
            </div>
            <div class="bg-green-50 p-4 rounded-lg border-l-4 border-green-400">
              <p class="text-base font-semibold text-green-800 mb-1">Tool Usage Gain</p>
              <p class="text-2xl font-bold text-green-900">+2% EM</p>
            </div>
          </div>
        </div>

        <!-- GUI Agents -->
        <div class="border border-gray-200 rounded-xl p-8 bg-white shadow-sm hover:shadow-md transition-shadow">
          <div class="text-center mb-6">
            <div class="w-16 h-16 bg-gray-100 rounded-full flex items-center justify-center mx-auto mb-4">
              <svg class="w-8 h-8 text-gray-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"></path>
              </svg>
            </div>
            <h3 class="text-xl font-semibold text-gray-800 mb-2">GUI Agents</h3>
          </div>
          <div class="space-y-4">
            <div class="bg-gray-50 p-4 rounded-lg border-l-4 border-gray-400">
              <p class="text-base font-semibold text-gray-800 mb-1">UI-TARS & ShowUI</p>
              <p class="text-2xl font-bold text-gray-900">0.00% EM</p>
            </div>
            <p class="text-base text-gray-600 leading-relaxed">Poor performance with LM maxing at 12.80%, reflecting low interaction capability.</p>
          </div>
        </div>
      </div>

      <!-- Model Results with Tab System -->
      <div class="tabs-container">
        <div class="tab-nav">
          <button class="tab-button active" onclick="showTab('vlms')">
            <svg class="tab-icon vlms" fill="none" stroke="currentColor" viewBox="0 0 24 24" style="width: 18px; height: 18px;">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"></path>
            </svg>
            VLMs Leaderboard
          </button>
          <button class="tab-button" onclick="showTab('agentic')">
            <svg class="tab-icon agentic" fill="none" stroke="currentColor" viewBox="0 0 24 24" style="width: 18px; height: 18px;">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M11 4a2 2 0 114 0v1a1 1 0 001 1h3a1 1 0 011 1v3a1 1 0 01-1 1h-1a2 2 0 100 4h1a1 1 0 011 1v3a1 1 0 01-1 1h-3a1 1 0 01-1-1v-1a2 2 0 10-4 0v1a1 1 0 01-1 1H7a1 1 0 01-1-1v-3a1 1 0 00-1-1H4a1 1 0 01-1-1V9a1 1 0 011-1h1a2 2 0 100-4H4a1 1 0 01-1-1V4a1 1 0 011-1h3a1 1 0 001-1z"></path>
            </svg>
            Agentic Models
          </button>
          <button class="tab-button" onclick="showTab('gui')">
            <svg class="tab-icon gui" fill="none" stroke="currentColor" viewBox="0 0 24 24" style="width: 18px; height: 18px;">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"></path>
            </svg>
            GUI Models
          </button>
        </div>

        <!-- VLMs Tab Content -->
        <div id="vlms-tab" class="tab-content active">
          <div class="table-wrapper">
            <table style="width:100%; border-collapse: collapse; font-family: sans-serif; font-size: 14px;">
                <caption style="text-align: left; font-weight: bold; margin-bottom: 8px; padding: 16px 16px 8px 16px;">
                  Table 1: Performance of Vision-Language Models on OWLViz
                </caption>
                <thead>
                    <tr>
                        <th></th>
                        <th>Model</th>
                        <th>Language Model</th>
                        <th>Vision Model</th>
                        <th>EM (%) ↓</th>
                        <th>LM (%)</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Small Open Source -->
                    <tr><td rowspan="11" class="group-label">Small Open Source</td><td><span class="model-name"><span class="model-icon deepseek-icon"></span>DeepSeek-VL2-small</span></td><td>-</td><td>-</td><td>11.16</td><td>12.75</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon deepseek-icon"></span>DeepSeek-VL2</span></td><td>-</td><td>-</td><td>11.16</td><td>14.34</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon qwen-icon"></span>Qwen2-VL-7B-Instruct</span></td><td>Qwen2-7B</td><td>QwenViT</td><td>12.75</td><td>17.93</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon qwen-icon"></span>Qwen2.5-VL-7B-Instruct</span></td><td>Qwen2.5-7B</td><td>QwenViT</td><td>13.94</td><td>19.52</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon internvl-icon"></span>InternVL3-8B</span></td><td>Qwen2.5-7B</td><td>InternViT-300M-v2.5</td><td>14.34</td><td class="underline">21.12</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon mistral-icon"></span>LLaVa-v1.6-mistral-7B</span></td><td>Mistral-7B</td><td>CLIP ViT-L/14</td><td>14.74</td><td>15.54</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon llama-icon"></span>Llama-3.2-11B-Vision-Instruct</span></td><td>Llama-3.1-8B</td><td>-</td><td>14.74</td><td class="bold">25.10</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon internvl-icon"></span>InternVL2.5-8B</span></td><td>InternLM2.5-7B</td><td>InternViT-300M-v2.5</td><td>14.74</td><td>18.73</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon vicuna-icon"></span>LLaVa-v1.5-13B</span></td><td>Vicuna-v1.5-13B</td><td>CLIP ViT-L/14</td><td>16.33</td><td>16.33</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon molmo-icon"></span>Molmo-7B-D-0924</span></td><td>Qwen2-7B</td><td>CLIP ViT-L/14</td><td class="underline">17.13</td><td>20.32</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon vicuna-icon"></span>LLaVa-v1.5-7B</span></td><td>Vicuna-v1.5-7B</td><td>CLIP ViT-L/14</td><td class="bold">18.33</td><td>19.92</td></tr>

                    <!-- Group Separator -->
                    <tr class="group-separator"><td colspan="6"></td></tr>

                    <!-- Large Open Source -->
                    <tr><td rowspan="9" class="group-label">Large Open Source</td><td><span class="model-name"><span class="model-icon qwen-icon"></span>Qwen2.5-VL-32B-Instruct</span></td><td>Qwen2.5-32B</td><td>QwenViT</td><td>2.79</td><td class="underline">25.90</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon internvl-icon"></span>InternVL2.5-38B</span></td><td>Qwen-2.5-32B</td><td>InternViT-6B-v2.5</td><td>13.94</td><td>19.52</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon internvl-icon"></span>InternVL3-78B</span></td><td>Qwen2.5-72B</td><td>InternViT-6B-v2.5</td><td>15.54</td><td>20.72</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon molmo-icon"></span>Molmo-72B-0924</span></td><td>Qwen2-72B</td><td>CLIP ViT-L/14</td><td>15.94</td><td>22.71</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon internvl-icon"></span>InternVL2.5-78B</span></td><td>Qwen-2.5-72B</td><td>InternViT-6B-v2.5</td><td>15.94</td><td>21.91</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon internvl-icon"></span>InternVL3-38B</span></td><td>Qwen2.5-32B</td><td>InternViT-6B-v2.5</td><td>16.73</td><td>23.11</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon qwen-icon"></span>Qwen2-VL-72B-Instruct</span></td><td>Qwen2-72B</td><td>QwenViT</td><td>19.92</td><td>25.90</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon qwen-icon"></span>Qwen2.5-VL-72B-Instruct</span></td><td>Qwen2.5-72B</td><td>QwenViT</td><td class="underline">20.32</td><td class="bold">26.29</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon llama-icon"></span>Llama-3.2-90B-Vision-Instruct</span></td><td>Llama-3.1-70B</td><td>-</td><td class="bold">20.72</td><td>24.70</td></tr>

                    <!-- Group Separator -->
                    <tr class="group-separator"><td colspan="6"></td></tr>

                    <!-- Proprietary -->
                    <tr><td rowspan="7" class="group-label">Proprietary</td><td><span class="model-name"><span class="model-icon claude-icon"></span>Claude-3-5-sonnet-20241022</span></td><td>-</td><td>-</td><td>11.55</td><td>19.92</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon openai-icon"></span>GPT-4V</span></td><td>-</td><td>-</td><td>14.34</td><td>20.00</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon gemini-icon"></span>Gemini-2.5-Flash</span></td><td>-</td><td>-</td><td>15.54</td><td class="underline">25.50</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon openai-icon"></span>GPT-4o</span></td><td>-</td><td>-</td><td>16.33</td><td>19.52</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon gemini-icon"></span>Gemini-1.5-Pro</span></td><td>-</td><td>-</td><td class="underline">19.52</td><td>21.91</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon gemini-icon"></span>Gemini-2.0-Flash</span></td><td>-</td><td>-</td><td class="bold">21.51</td><td>24.30</td></tr>
                    <tr><td><span class="model-name"><span class="model-icon gemini-icon"></span>Gemini-2.5-Pro</span></td><td>-</td><td>-</td><td class="bold">21.51</td><td class="bold">27.09</td></tr>
                </tbody>
            </table>
            <div class="note">
                <p>
                    *Model performance breakdown into 3 groups ordered by EM performance.
                    The best and second-best of each group are <strong>bolded</strong> and
                    <span style="text-decoration: underline;">underlined</span>, respectively.
                </p>
                *GPT-4o is used as the judge model to evaluate semantic equivalence between predicted and ground truth answers, producing the LLM-based Match (LM) accuracy metric.
            </div>
          </div>
        </div>

        <!-- Agentic Models Tab Content -->
        <div id="agentic-tab" class="tab-content">
          <div class="table-wrapper">
            <table style="width:100%; border-collapse: collapse; font-family: sans-serif; font-size: 14px;">
              <caption style="text-align: left; font-weight: bold; margin-bottom: 8px; padding: 16px 16px 8px 16px;">
                Table 2: Performance of Agentic models with tool-uses
              </caption>
              <thead>
                <tr>
                  <th>Model</th>
                  <th>MLLM</th>
                  <th>EM (%)</th>
                  <th>LM (%)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>LLaVa-Plus</td>
                  <td>gpt-4o-2024-11-20</td>
                  <td>0.00</td>
                  <td>2.50</td>
                </tr>
                <tr>
                  <td>ViperGPT</td>
                  <td>gpt-4o-2024-11-20</td>
                  <td>7.56</td>
                  <td>12.35</td>
                </tr>
                <tr>
                  <td>GPT4Tools</td>
                  <td>vicuna-7b-v1.5</td>
                  <td>11.15</td>
                  <td>14.34</td>
                </tr>
                <tr>
                  <td>HYDRA</td>
                  <td>gpt-4o-2024-11-20</td>
                  <td>10.75</td>
                  <td>12.35</td>
                </tr>
                <tr>
                  <td>HF Agent</td>
                  <td>gpt-4o-2024-11-20</td>
                  <td><strong>18.32</strong></td>
                  <td><span style="text-decoration: underline;">24.08</span></td>
                </tr>
                <tr>
                  <td>DynaSaur</td>
                  <td>gpt-4o-2024-11-20</td>
                  <td><span style="text-decoration: underline;">16.23</span></td>
                  <td><strong>26.67</strong></td>
                </tr>
              </tbody>
            </table>
            <div class="note">
              <p>
                *The best and second-best of each group are
                <strong>bolded</strong> and <span style="text-decoration: underline;">underlined</span>, respectively.
              </p>
            </div>
          </div>
        </div>

        <!-- GUI Models Tab Content -->
        <div id="gui-tab" class="tab-content">
          <div class="table-wrapper">
            <table style="width:100%; border-collapse: collapse; font-family: sans-serif; font-size: 14px;">
              <caption style="text-align: left; font-weight: bold; margin-bottom: 8px; padding: 16px 16px 8px 16px;">
                Table 3: Performance of GUI-based models
              </caption>
              <thead style="background-color: #f3f4f6;">
                <tr>
                  <th style="border: 1px solid #ddd; padding: 8px;"><strong>Model</strong></th>
                  <th style="border: 1px solid #ddd; padding: 8px;"><strong>EM</strong></th>
                  <th style="border: 1px solid #ddd; padding: 8px;"><strong>LM</strong></th>
                  <th style="border: 1px solid #ddd; padding: 8px;"><strong>Click</strong></th>
                  <th style="border: 1px solid #ddd; padding: 8px;"><strong>Hover</strong></th>
                  <th style="border: 1px solid #ddd; padding: 8px;"><strong>Scroll</strong></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="border: 1px solid #ddd; padding: 8px;">UI-TARS</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">0.00</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">12.31</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">0.91</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">0.51</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">0.68</td>
                </tr>
                <tr>
                  <td style="border: 1px solid #ddd; padding: 8px;">ShowUI</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">0.00</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">12.80</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">0.97</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">0.19</td>
                  <td style="border: 1px solid #ddd; padding: 8px;">0.10</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>


      <!-- Key Insights -->
      <div class="bg-gray-50 rounded-xl p-8 mb-10 border border-gray-200">
        <h3 class="text-2xl font-semibold mb-8 text-gray-800 flex items-center justify-center">
          <svg class="w-6 h-6 mr-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.663 17h4.673M12 3v1m6.364 1.636l-.707.707M21 12h-1M4 12H3m3.343-5.657l-.707-.707m2.828 9.9a5 5 0 117.072 0l-.548.547A3.374 3.374 0 0014 18.469V19a2 2 0 11-4 0v-.531c0-.895-.356-1.754-.988-2.386l-.548-.547z"></path>
          </svg>
          Key Research Insights
        </h3>
        <div class="grid lg:grid-cols-2 gap-6">
          <div class="flex items-start space-x-4 p-4 bg-white rounded-lg border border-gray-100">
            <div class="w-3 h-3 bg-red-500 rounded-full mt-2 flex-shrink-0"></div>
            <p class="text-base leading-relaxed text-gray-700">Even state-of-the-art VLMs struggle significantly with open-world reasoning tasks</p>
          </div>
          <div class="flex items-start space-x-4 p-4 bg-white rounded-lg border border-gray-100">
            <div class="w-3 h-3 bg-yellow-500 rounded-full mt-2 flex-shrink-0"></div>
            <p class="text-base leading-relaxed text-gray-700">Tool integration provides measurable but modest improvements in performance</p>
          </div>
          <div class="flex items-start space-x-4 p-4 bg-white rounded-lg border border-gray-100">
            <div class="w-3 h-3 bg-gray-500 rounded-full mt-2 flex-shrink-0"></div>
            <p class="text-base leading-relaxed text-gray-700">GUI-based approaches currently lack the sophistication for complex multi-modal tasks</p>
          </div>
          <div class="flex items-start space-x-4 p-4 bg-white rounded-lg border border-gray-100">
            <div class="w-3 h-3 bg-blue-500 rounded-full mt-2 flex-shrink-0"></div>
            <p class="text-base leading-relaxed text-gray-700">Large performance gap between human and AI capabilities highlights research opportunities</p>
          </div>
        </div>
      </div>

      <!-- Qualitative Analysis -->
      <div class="border border-gray-200 rounded-xl p-8 mb-10 bg-gradient-to-r from-gray-50 to-gray-100">
        <h3 class="text-2xl font-semibold mb-8 text-gray-800 text-center">Qualitative Analysis</h3>
        
        <!-- Question & Answer + Image Layout (Top Row) -->
        <div class="flex flex-col lg:flex-row gap-4 mb-6">
          <!-- Question & Answer Box (Left) -->
          <div class="lg:w-2/5">
            <div class="bg-cyan-50 border-2 border-cyan-300 rounded-lg p-4 h-56 flex flex-col">
              <h4 class="font-semibold text-cyan-800 mb-3 text-center text-sm bg-cyan-200 py-1 px-2 rounded">Question & Answer</h4>
              <div class="text-xs text-gray-700 leading-relaxed flex-1">
                <p class="mb-3 font-medium">What is the name of the shop that is located across the street from the lot for sale in this photo? Provide an answer in fewer than 3 words</p>
                <div class="mt-4">
                  <p class="font-semibold text-xs">Any of the following answers are acceptable:</p>
                  <p class="italic text-xs">Wheat Bay; Uniquely Chengdu; Wheat Bay Uniquely Chengdu</p>
                </div>
              </div>
            </div>
          </div>
          
          <!-- Image (Right) -->
          <div class="lg:w-3/5">
            <div class="bg-white rounded-lg border border-gray-200 p-2 h-56">
              <img src="assets/images/q3-map.png" alt="Map showing shop location" class="w-full h-full object-cover rounded">
            </div>
          </div>
        </div>
        
        <!-- Model Responses Row (Bottom) -->
        <div class="grid grid-cols-1 md:grid-cols-3 gap-3">
          <!-- Gemini Response -->
          <div class="bg-blue-50 border-2 border-blue-400 rounded-lg p-3 h-24 flex flex-col overflow-hidden response-box">
            <h4 class="font-semibold text-blue-800 mb-1 text-xs bg-blue-200 py-1 px-2 rounded text-center flex-shrink-0">Gemini</h4>
            <div class="text-xs text-gray-700 italic leading-tight flex-1 overflow-hidden response-content">
              <p class="truncate">...Identify the shop across Alder Street from the for-sale lot.</p>
              <p class="font-semibold mt-1 truncate">Not identifiable</p>
            </div>
          </div>
          
          <!-- DynaSaur Response -->
          <div class="bg-green-50 border-2 border-green-600 rounded-lg p-3 h-24 flex flex-col overflow-hidden response-box">
            <h4 class="font-semibold text-green-800 mb-1 text-xs bg-green-200 py-1 px-2 rounded text-center flex-shrink-0">DynaSaur</h4>
            <div class="text-xs text-gray-700 italic leading-tight flex-1 overflow-hidden response-content">
              <p class="line-clamp-3">The name of the shop across the street is visible in the image. It is "Starbucks Coffee".</p>
            </div>
          </div>
          
          <!-- ShowUI Response -->
          <div class="bg-red-50 border-2 border-red-400 rounded-lg p-3 h-24 flex flex-col overflow-hidden response-box">
            <h4 class="font-semibold text-red-800 mb-1 text-xs bg-red-200 py-1 px-2 rounded text-center flex-shrink-0">ShowUI</h4>
            <div class="text-xs text-gray-700 italic leading-tight flex-1 overflow-hidden response-content">
              <p class="truncate">I need to...</p>
              <p class="truncate">Action: Scroll</p>
              <p class="font-semibold truncate">No answer</p>
            </div>
          </div>
        </div>
        
        <!-- Caption -->
        <div class="mt-6 text-left">
          <p class="text-sm text-gray-600 leading-relaxed max-w-4xl mx-auto">
            Figure: <strong>Qualitative results comparing different model capabilities on OWLViz.</strong> 
            Results demonstrate varying capabilities across model types: Gemini (vanilla VLM) fails to identify the target, 
            DynaSaur (tool-calling agent) produces an incorrect answer despite external search capabilities, 
            and ShowUI (GUI agent) provides no answer.
          </p>
        </div>
      </div>
    </section>

    <section class="mt-16">
      <h2 class="heading">Limitations</h2>
      <div class="underline-gradient"></div>
      <div class="mx-8 md:mx-16">
        <p class="text-center text-lg leading-relaxed">
          The current evaluation method transforms questions into constrained response types (e.g., multiple-choice, yes/no, numerical, short text) to enable exact-match evaluation. While this ensures consistency, it may increase the likelihood of correct responses by narrowing the output space, potentially overestimating model performance compared to free-form answers.
        </p>
      </div>
    </section>

    <section class="mt-16">
      <h2 class="heading">Accessing the Dataset</h2>
      <div class="underline-gradient"></div>
      <div class="mx-8 md:mx-16">
        <p class="text-center text-lg leading-relaxed">
          The OWLViz dataset is currently <strong>kept private to minimize data contamination</strong>. Additional information on how to access the data is available upon request.
        </p>
      </div>
    </section>

    <section class="mt-16">
      <h2 class="heading">Acknowledgments</h2>
      <div class="underline-gradient"></div>
      <div class="mx-8 md:mx-16">
        <p class="text-center text-lg leading-relaxed">
          We thank Adobe Research for their financial and technical support.
        </p>
      </div>
    </section>

    <section class="mt-16">
      <h2 class="heading">Citation</h2>
      <div class="underline-gradient"></div>
      <div class="bg-white border border-gray-300 rounded-lg p-6 max-w-4xl mx-auto">
        <div class="flex justify-between items-center mb-4">
          <h3 class="text-lg font-semibold text-gray-800">BibTeX</h3>
          <button 
            onclick="copyToClipboard()" 
            class="bg-blue-500 hover:bg-blue-600 text-white px-4 py-2 rounded text-sm font-medium transition-colors duration-200 flex items-center gap-2"
            id="copy-button"
          >
            <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5H7a2 2 0 00-2 2v10a2 2 0 002 2h8a2 2 0 002-2V7a2 2 0 00-2-2h-2M9 5a2 2 0 002 2h2a2 2 0 002-2M9 5a2 2 0 012-2h2a2 2 0 012 2"></path>
            </svg>
            Copy
          </button>
        </div>
        <pre id="citation-text" class="bg-gray-50 p-4 rounded text-sm overflow-x-auto border border-gray-200 font-mono leading-relaxed whitespace-pre-wrap">@article{nguyen2025owlviz,
  title={OWLViz: An Open-World Benchmark for Visual Question Answering},
  author={Nguyen, Thuy and Nguyen, Dang and Nguyen, Hoang and 
          Luong, Thuan and Dang, Long Hoang and Lai, Viet Dac},
  journal={arXiv preprint arXiv:2503.07631},
  year={2025}
}</pre>
      </div>
      
      <script src="script.js"></script>
    </section>
  </div>
</body>
</html>